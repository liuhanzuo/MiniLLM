"""
test_grpo.py

GRPOæ ¸å¿ƒåŠŸèƒ½çš„å•å…ƒæµ‹è¯•
"""

import sys
import torch


def test_orm_advantages():
    """æµ‹è¯•ORMæ¨¡å¼çš„ä¼˜åŠ¿å‡½æ•°è®¡ç®—"""
    print("\n" + "="*60)
    print("æµ‹è¯• 1: ORMä¼˜åŠ¿å‡½æ•°è®¡ç®—")
    print("="*60)
    
    # å¯¼å…¥å‡½æ•°
    sys.path.insert(0, 'class/lec15')
    from grpo_demo import compute_orm_advantages
    
    # æµ‹è¯•ç”¨ä¾‹1ï¼šç®€å•æƒ…å†µ
    rewards = [1.0, 2.0, 3.0, 4.0]
    advantages = compute_orm_advantages(rewards)
    
    print(f"è¾“å…¥å¥–åŠ±: {rewards}")
    print(f"è¾“å‡ºä¼˜åŠ¿: {[f'{a:.4f}' for a in advantages]}")
    
    # éªŒè¯ï¼šå‡å€¼åº”è¯¥æ¥è¿‘0ï¼Œæ ‡å‡†å·®åº”è¯¥æ¥è¿‘1
    mean_adv = sum(advantages) / len(advantages)
    std_adv = (sum((a - mean_adv)**2 for a in advantages) / len(advantages)) ** 0.5
    
    print(f"ä¼˜åŠ¿å‡å€¼: {mean_adv:.6f} (åº”æ¥è¿‘0)")
    print(f"ä¼˜åŠ¿æ ‡å‡†å·®: {std_adv:.6f} (åº”æ¥è¿‘1)")
    
    assert abs(mean_adv) < 1e-6, "ä¼˜åŠ¿å‡å€¼åº”è¯¥æ¥è¿‘0"
    print("âœ… ORMä¼˜åŠ¿å‡½æ•°æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•ç”¨ä¾‹2ï¼šç›¸åŒå¥–åŠ±
    rewards_same = [2.0, 2.0, 2.0, 2.0]
    advantages_same = compute_orm_advantages(rewards_same)
    print(f"\nç›¸åŒå¥–åŠ±: {rewards_same}")
    print(f"è¾“å‡ºä¼˜åŠ¿: {[f'{a:.4f}' for a in advantages_same]}")
    print("âœ… ç›¸åŒå¥–åŠ±æƒ…å†µæµ‹è¯•é€šè¿‡")


def test_prm_advantages():
    """æµ‹è¯•PRMæ¨¡å¼çš„ä¼˜åŠ¿å‡½æ•°è®¡ç®—"""
    print("\n" + "="*60)
    print("æµ‹è¯• 2: PRMä¼˜åŠ¿å‡½æ•°è®¡ç®—")
    print("="*60)
    
    from grpo_demo import compute_prm_advantages
    
    # æµ‹è¯•ç”¨ä¾‹ï¼š3ä¸ªè¾“å‡ºï¼Œä¸åŒæ­¥æ•°
    step_rewards = [
        [1.0, 1.5, 2.0, 2.5],      # è¾“å‡º1ï¼š4æ­¥
        [0.5, 1.0, 1.5],            # è¾“å‡º2ï¼š3æ­¥
        [1.5, 2.0, 2.5, 3.0, 3.5]  # è¾“å‡º3ï¼š5æ­¥
    ]
    
    advantages = compute_prm_advantages(step_rewards)
    
    print("è¾“å…¥æ­¥éª¤å¥–åŠ±:")
    for i, rewards in enumerate(step_rewards):
        print(f"  è¾“å‡º{i+1}: {rewards}")
    
    print("\nè¾“å‡ºæ­¥éª¤ä¼˜åŠ¿:")
    for i, advs in enumerate(advantages):
        print(f"  è¾“å‡º{i+1}: {[f'{a:.4f}' for a in advs]}")
    
    # éªŒè¯ï¼šæ¯ä¸€æ­¥çš„ä¼˜åŠ¿åœ¨æ‰€æœ‰è¾“å‡ºä¸­åº”è¯¥æ˜¯å½’ä¸€åŒ–çš„
    max_steps = max(len(r) for r in step_rewards)
    for t in range(max_steps):
        advs_at_t = []
        for i, advs in enumerate(advantages):
            if t < len(advs):
                advs_at_t.append(advs[t])
        
        if len(advs_at_t) > 1:
            mean_t = sum(advs_at_t) / len(advs_at_t)
            print(f"æ­¥éª¤{t}çš„ä¼˜åŠ¿å‡å€¼: {mean_t:.6f} (åº”æ¥è¿‘0)")
            assert abs(mean_t) < 0.1, f"æ­¥éª¤{t}çš„ä¼˜åŠ¿å‡å€¼åº”è¯¥æ¥è¿‘0"
    
    print("âœ… PRMä¼˜åŠ¿å‡½æ•°æµ‹è¯•é€šè¿‡")


def test_grpo_loss():
    """æµ‹è¯•GRPOæŸå¤±è®¡ç®—"""
    print("\n" + "="*60)
    print("æµ‹è¯• 3: GRPOæŸå¤±è®¡ç®—")
    print("="*60)
    
    from grpo_demo import grpo_loss
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    seq_len = 10
    
    # å½“å‰ç­–ç•¥çš„logæ¦‚ç‡
    log_probs = torch.randn(batch_size, seq_len)
    
    # æ—§ç­–ç•¥çš„logæ¦‚ç‡ï¼ˆç¨å¾®ä¸åŒï¼‰
    old_log_probs = log_probs + torch.randn(batch_size, seq_len) * 0.1
    
    # ä¼˜åŠ¿å‡½æ•°ï¼ˆæœ‰æ­£æœ‰è´Ÿï¼‰
    advantages = torch.randn(batch_size, seq_len)
    
    # è®¡ç®—æŸå¤±
    loss = grpo_loss(log_probs, old_log_probs, advantages, clip_epsilon=0.2)
    
    print(f"Log probs shape: {log_probs.shape}")
    print(f"Old log probs shape: {old_log_probs.shape}")
    print(f"Advantages shape: {advantages.shape}")
    print(f"GRPO Loss: {loss.item():.6f}")
    
    # éªŒè¯ï¼šæŸå¤±åº”è¯¥æ˜¯æ ‡é‡ä¸”ä¸ºæ­£
    assert loss.dim() == 0, "æŸå¤±åº”è¯¥æ˜¯æ ‡é‡"
    assert loss.item() >= 0 or loss.item() < 0, "æŸå¤±åº”è¯¥æ˜¯æœ‰é™å€¼"
    
    print("âœ… GRPOæŸå¤±è®¡ç®—æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•clipæ•ˆæœ
    print("\næµ‹è¯•clipæ•ˆæœ:")
    
    # æƒ…å†µ1ï¼šratioåœ¨clipèŒƒå›´å†…
    log_probs_in = torch.tensor([[0.0]])
    old_log_probs_in = torch.tensor([[0.0]])
    advantages_in = torch.tensor([[1.0]])
    loss_in = grpo_loss(log_probs_in, old_log_probs_in, advantages_in, clip_epsilon=0.2)
    print(f"  Ratio=1.0 (åœ¨èŒƒå›´å†…), Loss: {loss_in.item():.6f}")
    
    # æƒ…å†µ2ï¼šratioè¶…å‡ºclipèŒƒå›´
    log_probs_out = torch.tensor([[1.0]])
    old_log_probs_out = torch.tensor([[0.0]])
    advantages_out = torch.tensor([[1.0]])
    loss_out = grpo_loss(log_probs_out, old_log_probs_out, advantages_out, clip_epsilon=0.2)
    print(f"  Ratio={torch.exp(torch.tensor(1.0)).item():.2f} (è¶…å‡ºèŒƒå›´), Loss: {loss_out.item():.6f}")
    
    print("âœ… Clipæ•ˆæœæµ‹è¯•é€šè¿‡")


def test_kl_divergence():
    """æµ‹è¯•KLæ•£åº¦è®¡ç®—"""
    print("\n" + "="*60)
    print("æµ‹è¯• 4: KLæ•£åº¦è®¡ç®—")
    print("="*60)
    
    from grpo_demo import compute_kl_divergence
    
    # æµ‹è¯•ç”¨ä¾‹1ï¼šç›¸åŒåˆ†å¸ƒï¼ŒKLåº”è¯¥æ¥è¿‘0
    log_probs_same = torch.randn(10, 20)
    kl_same = compute_kl_divergence(log_probs_same, log_probs_same)
    print(f"ç›¸åŒåˆ†å¸ƒçš„KLæ•£åº¦: {kl_same.item():.6f} (åº”æ¥è¿‘0)")
    assert abs(kl_same.item()) < 1e-5, "ç›¸åŒåˆ†å¸ƒçš„KLæ•£åº¦åº”è¯¥æ¥è¿‘0"
    
    # æµ‹è¯•ç”¨ä¾‹2ï¼šä¸åŒåˆ†å¸ƒ
    log_probs_1 = torch.randn(10, 20)
    log_probs_2 = log_probs_1 + torch.randn(10, 20) * 0.5
    kl_diff = compute_kl_divergence(log_probs_1, log_probs_2)
    print(f"ä¸åŒåˆ†å¸ƒçš„KLæ•£åº¦: {kl_diff.item():.6f} (åº”å¤§äº0)")
    assert kl_diff.item() >= 0, "KLæ•£åº¦åº”è¯¥éè´Ÿ"
    
    print("âœ… KLæ•£åº¦è®¡ç®—æµ‹è¯•é€šè¿‡")


def test_reward_model_wrapper():
    """æµ‹è¯•Reward ModelåŒ…è£…å™¨"""
    print("\n" + "="*60)
    print("æµ‹è¯• 5: Reward ModelåŒ…è£…å™¨")
    print("="*60)
    
    from grpo_demo import RewardModelWrapper
    
    # æµ‹è¯•ORMæ¨¡å¼ï¼ˆä½¿ç”¨dummy rewardsï¼‰
    print("\næµ‹è¯•ORMæ¨¡å¼:")
    rm_orm = RewardModelWrapper(model_path=None, mode="orm", device="cpu")
    
    batch_size = 4
    seq_len = 20
    input_ids = torch.randint(0, 1000, (batch_size, seq_len))
    attention_mask = torch.ones(batch_size, seq_len)
    
    rewards_orm = rm_orm.get_rewards_orm(input_ids, attention_mask)
    print(f"  è¾“å…¥shape: {input_ids.shape}")
    print(f"  ORMå¥–åŠ±: {[f'{r:.4f}' for r in rewards_orm]}")
    assert len(rewards_orm) == batch_size, "ORMåº”è¯¥è¿”å›batch_sizeä¸ªå¥–åŠ±"
    
    # æµ‹è¯•PRMæ¨¡å¼
    print("\næµ‹è¯•PRMæ¨¡å¼:")
    rm_prm = RewardModelWrapper(model_path=None, mode="prm", device="cpu")
    
    rewards_prm = rm_prm.get_rewards_prm(input_ids, attention_mask)
    print(f"  è¾“å…¥shape: {input_ids.shape}")
    print(f"  PRMå¥–åŠ±æ•°é‡: {len(rewards_prm)}")
    for i, step_rewards in enumerate(rewards_prm[:2]):  # åªæ˜¾ç¤ºå‰2ä¸ª
        print(f"  è¾“å‡º{i+1}æ­¥éª¤å¥–åŠ±: {[f'{r:.4f}' for r in step_rewards[:5]]}... (å…±{len(step_rewards)}æ­¥)")
    
    assert len(rewards_prm) == batch_size, "PRMåº”è¯¥è¿”å›batch_sizeä¸ªè¾“å‡ºçš„å¥–åŠ±"
    
    print("âœ… Reward ModelåŒ…è£…å™¨æµ‹è¯•é€šè¿‡")


def test_integration():
    """é›†æˆæµ‹è¯•ï¼šå®Œæ•´çš„GRPOæµç¨‹"""
    print("\n" + "="*60)
    print("æµ‹è¯• 6: é›†æˆæµ‹è¯•")
    print("="*60)
    
    from grpo_demo import compute_orm_advantages, grpo_loss, compute_kl_divergence
    
    # æ¨¡æ‹Ÿä¸€ä¸ªå®Œæ•´çš„GRPOæ›´æ–°æ­¥éª¤
    group_size = 4
    seq_len = 10
    
    print(f"æ¨¡æ‹Ÿåœºæ™¯: {group_size}ä¸ªè¾“å‡ºï¼Œæ¯ä¸ª{seq_len}ä¸ªtoken")
    
    # 1. ç”Ÿæˆå¥–åŠ±
    rewards = [float(i) + torch.randn(1).item() * 0.5 for i in range(group_size)]
    print(f"\n1. å¥–åŠ±: {[f'{r:.4f}' for r in rewards]}")
    
    # 2. è®¡ç®—ä¼˜åŠ¿
    advantages = compute_orm_advantages(rewards)
    print(f"2. ä¼˜åŠ¿: {[f'{a:.4f}' for a in advantages]}")
    
    # 3. å‡†å¤‡logæ¦‚ç‡
    log_probs = torch.randn(group_size, seq_len)
    old_log_probs = log_probs + torch.randn(group_size, seq_len) * 0.1
    advantages_tensor = torch.tensor(advantages).unsqueeze(1).expand(-1, seq_len)
    
    # 4. è®¡ç®—æŸå¤±
    policy_loss = grpo_loss(log_probs, old_log_probs, advantages_tensor, clip_epsilon=0.2)
    kl = compute_kl_divergence(log_probs, old_log_probs)
    
    print(f"3. Policy Loss: {policy_loss.item():.6f}")
    print(f"4. KL Divergence: {kl.item():.6f}")
    
    # 5. æ€»æŸå¤±
    kl_coef = 0.1
    total_loss = policy_loss + kl_coef * kl
    print(f"5. Total Loss: {total_loss.item():.6f}")
    
    print("\nâœ… é›†æˆæµ‹è¯•é€šè¿‡")


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*60)
    print("ğŸ§ª GRPOå•å…ƒæµ‹è¯•")
    print("="*60)
    
    try:
        test_orm_advantages()
        test_prm_advantages()
        test_grpo_loss()
        test_kl_divergence()
        test_reward_model_wrapper()
        test_integration()
        
        print("\n" + "="*60)
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("="*60)
        
    except Exception as e:
        print("\n" + "="*60)
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print("="*60)
        import traceback
        traceback.print_exc()
        return False
    
    return True


if __name__ == '__main__':
    success = run_all_tests()
    sys.exit(0 if success else 1)

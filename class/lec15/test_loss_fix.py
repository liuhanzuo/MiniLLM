#!/usr/bin/env python3
"""
test_loss_fix.py

éªŒè¯GRPO lossä¿®å¤æ˜¯å¦ç”Ÿæ•ˆ
"""

import torch
import numpy as np


def test_advantage_computation():
    """æµ‹è¯•ä¼˜åŠ¿å‡½æ•°è®¡ç®—æ˜¯å¦æ­£ç¡®"""
    print("=" * 60)
    print("æµ‹è¯•1: ä¼˜åŠ¿å‡½æ•°è®¡ç®—")
    print("=" * 60)
    
    # å¯¼å…¥ä¿®å¤åçš„å‡½æ•°
    import sys
    sys.path.insert(0, '/apdcephfs/pig_data/MiniLLM/class/lec15')
    from grpo_demo import compute_orm_advantages
    
    # æµ‹è¯•ç”¨ä¾‹1: æ­£å¸¸æƒ…å†µ
    print("\nâœ… æµ‹è¯•ç”¨ä¾‹1: æ­£å¸¸å¥–åŠ±å€¼")
    rewards1 = [0.5, 1.0, 1.5, 2.0]
    adv1 = compute_orm_advantages(rewards1)
    print(f"  Rewards: {rewards1}")
    print(f"  Advantages: {[f'{a:.4f}' for a in adv1]}")
    print(f"  Range: [{min(adv1):.4f}, {max(adv1):.4f}]")
    assert all(-15 < a < 15 for a in adv1), "ä¼˜åŠ¿å€¼åº”è¯¥åœ¨åˆç†èŒƒå›´å†…"
    print("  âœ… é€šè¿‡")
    
    # æµ‹è¯•ç”¨ä¾‹2: æ ‡å‡†å·®å¾ˆå°çš„æƒ…å†µï¼ˆä¿®å¤çš„å…³é”®ï¼‰
    print("\nâœ… æµ‹è¯•ç”¨ä¾‹2: æ ‡å‡†å·®å¾ˆå°ï¼ˆåº”è¯¥è¿”å›å…¨0ï¼‰")
    rewards2 = [0.1, 0.11, 0.12, 0.13]
    adv2 = compute_orm_advantages(rewards2)
    print(f"  Rewards: {rewards2}")
    print(f"  Std: {np.std(rewards2):.6f}")
    print(f"  Advantages: {[f'{a:.4f}' for a in adv2]}")
    assert all(abs(a) < 0.1 for a in adv2), "æ ‡å‡†å·®å¾ˆå°æ—¶ï¼Œä¼˜åŠ¿å€¼åº”è¯¥æ¥è¿‘0"
    print("  âœ… é€šè¿‡ï¼ˆä¿®å¤ç”Ÿæ•ˆï¼ï¼‰")
    
    # æµ‹è¯•ç”¨ä¾‹3: æç«¯æƒ…å†µ
    print("\nâœ… æµ‹è¯•ç”¨ä¾‹3: æç«¯å¥–åŠ±å€¼")
    rewards3 = [-10.0, -5.0, 5.0, 10.0]
    adv3 = compute_orm_advantages(rewards3)
    print(f"  Rewards: {rewards3}")
    print(f"  Advantages: {[f'{a:.4f}' for a in adv3]}")
    print(f"  Range: [{min(adv3):.4f}, {max(adv3):.4f}]")
    assert all(-15 < a < 15 for a in adv3), "å³ä½¿å¥–åŠ±æç«¯ï¼Œä¼˜åŠ¿å€¼ä¹Ÿåº”è¯¥è¢«clip"
    print("  âœ… é€šè¿‡")
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰ä¼˜åŠ¿å‡½æ•°æµ‹è¯•é€šè¿‡ï¼")
    print("=" * 60)


def test_dummy_rewards():
    """æµ‹è¯•dummy rewardsæ˜¯å¦åˆç†"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•2: Dummy Rewardsç”Ÿæˆ")
    print("=" * 60)
    
    import sys
    sys.path.insert(0, '/apdcephfs/pig_data/MiniLLM/class/lec15')
    from grpo_demo import RewardModelWrapper
    
    # åˆ›å»ºdummy reward model
    rm = RewardModelWrapper(model_path=None, mode="orm")
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    batch_size = 4
    seq_len = 20
    input_ids = torch.randint(0, 1000, (batch_size, seq_len))
    attention_mask = torch.ones(batch_size, seq_len)
    
    # è·å–ORMå¥–åŠ±
    print("\nâœ… æµ‹è¯•ORMå¥–åŠ±")
    rewards_orm = rm.get_rewards_orm(input_ids, attention_mask)
    print(f"  Rewards: {[f'{r:.4f}' for r in rewards_orm]}")
    print(f"  Mean: {np.mean(rewards_orm):.4f}")
    print(f"  Std: {np.std(rewards_orm):.4f}")
    assert np.std(rewards_orm) > 0.1, "å¥–åŠ±æ ‡å‡†å·®åº”è¯¥ > 0.1"
    print("  âœ… é€šè¿‡ï¼ˆæ ‡å‡†å·®è¶³å¤Ÿå¤§ï¼‰")
    
    # è·å–PRMå¥–åŠ±
    print("\nâœ… æµ‹è¯•PRMå¥–åŠ±")
    rm_prm = RewardModelWrapper(model_path=None, mode="prm")
    rewards_prm = rm_prm.get_rewards_prm(input_ids, attention_mask)
    print(f"  è¾“å‡ºæ•°é‡: {len(rewards_prm)}")
    print(f"  ç¬¬ä¸€ä¸ªè¾“å‡ºçš„æ­¥éª¤æ•°: {len(rewards_prm[0])}")
    print(f"  ç¬¬ä¸€ä¸ªè¾“å‡ºçš„å‰5æ­¥å¥–åŠ±: {[f'{r:.4f}' for r in rewards_prm[0][:5]]}")
    assert all(len(r) > 0 for r in rewards_prm), "æ¯ä¸ªè¾“å‡ºéƒ½åº”è¯¥æœ‰å¥–åŠ±"
    print("  âœ… é€šè¿‡")
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰Dummy Rewardsæµ‹è¯•é€šè¿‡ï¼")
    print("=" * 60)


def test_loss_computation():
    """æµ‹è¯•æŸå¤±è®¡ç®—æ˜¯å¦æ­£ç¡®"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•3: æŸå¤±è®¡ç®—")
    print("=" * 60)
    
    import sys
    sys.path.insert(0, '/apdcephfs/pig_data/MiniLLM/class/lec15')
    from grpo_demo import grpo_loss
    
    # æµ‹è¯•ç”¨ä¾‹1: æ­£å¸¸æƒ…å†µ
    print("\nâœ… æµ‹è¯•ç”¨ä¾‹1: æ­£å¸¸æŸå¤±è®¡ç®—")
    log_probs = torch.randn(100) * 0.1 - 2.0  # æ¨¡æ‹Ÿlogæ¦‚ç‡
    old_log_probs = log_probs + torch.randn(100) * 0.05  # ç¨å¾®ä¸åŒ
    advantages = torch.randn(100) * 0.5  # æ­£å¸¸ä¼˜åŠ¿å€¼
    
    loss = grpo_loss(log_probs, old_log_probs, advantages, clip_epsilon=0.2)
    print(f"  Loss: {loss.item():.4f}")
    assert 0 < loss.item() < 10, "æŸå¤±åº”è¯¥åœ¨åˆç†èŒƒå›´å†…"
    print("  âœ… é€šè¿‡")
    
    # æµ‹è¯•ç”¨ä¾‹2: æç«¯ä¼˜åŠ¿å€¼ï¼ˆä¿®å¤ååº”è¯¥è¢«clipï¼‰
    print("\nâœ… æµ‹è¯•ç”¨ä¾‹2: æç«¯ä¼˜åŠ¿å€¼")
    advantages_extreme = torch.tensor([100.0, -100.0, 50.0, -50.0] * 25)  # æç«¯å€¼
    loss_extreme = grpo_loss(log_probs, old_log_probs, advantages_extreme, clip_epsilon=0.2)
    print(f"  Loss: {loss_extreme.item():.4f}")
    # ç”±äºä¼˜åŠ¿å€¼è¢«clipï¼ŒæŸå¤±ä¸åº”è¯¥å¤ªå¤§
    assert loss_extreme.item() < 1000, "å³ä½¿ä¼˜åŠ¿å€¼æç«¯ï¼ŒæŸå¤±ä¹Ÿä¸åº”è¯¥çˆ†ç‚¸"
    print("  âœ… é€šè¿‡ï¼ˆä¿®å¤ç”Ÿæ•ˆï¼ï¼‰")
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰æŸå¤±è®¡ç®—æµ‹è¯•é€šè¿‡ï¼")
    print("=" * 60)


def test_padding_mask():
    """æµ‹è¯•padding maskæ˜¯å¦æ­£ç¡®"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•4: Padding Mask")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿåœºæ™¯
    print("\nâœ… æµ‹è¯•paddingä½ç½®è¿‡æ»¤")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    seq_len = 10
    pad_token_id = 50256  # GPT2çš„pad token
    
    # labels: [1, 2, 3, pad, pad, pad, ...]
    labels = torch.tensor([[1, 2, 3, pad_token_id, pad_token_id, 
                           pad_token_id, pad_token_id, pad_token_id, 
                           pad_token_id, pad_token_id]])
    
    # åˆ›å»ºmask
    labels_mask = (labels != pad_token_id).float()
    valid_positions = labels_mask.bool()
    
    print(f"  Labels: {labels[0].tolist()}")
    print(f"  Mask: {labels_mask[0].tolist()}")
    print(f"  æœ‰æ•ˆä½ç½®æ•°: {valid_positions.sum().item()}")
    
    assert valid_positions.sum().item() == 3, "åº”è¯¥æœ‰3ä¸ªæœ‰æ•ˆä½ç½®"
    print("  âœ… é€šè¿‡")
    
    # æµ‹è¯•åªå¯¹æœ‰æ•ˆä½ç½®è®¡ç®—æŸå¤±
    print("\nâœ… æµ‹è¯•æŸå¤±åªåœ¨æœ‰æ•ˆä½ç½®è®¡ç®—")
    log_probs = torch.randn(1, seq_len)
    
    # å…¨éƒ¨ä½ç½®çš„æŸå¤±
    loss_all = log_probs.mean()
    print(f"  å…¨éƒ¨ä½ç½®æŸå¤±: {loss_all.item():.4f}")
    
    # åªæœ‰æ•ˆä½ç½®çš„æŸå¤±
    loss_valid = log_probs[valid_positions].mean()
    print(f"  æœ‰æ•ˆä½ç½®æŸå¤±: {loss_valid.item():.4f}")
    
    # å®ƒä»¬åº”è¯¥ä¸åŒ
    assert abs(loss_all.item() - loss_valid.item()) > 0.01, "ä¸¤ç§è®¡ç®—æ–¹å¼åº”è¯¥ä¸åŒ"
    print("  âœ… é€šè¿‡ï¼ˆæ­£ç¡®è¿‡æ»¤äº†paddingï¼‰")
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰Padding Maskæµ‹è¯•é€šè¿‡ï¼")
    print("=" * 60)


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 60)
    print("ğŸ§ª GRPO Lossä¿®å¤éªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    try:
        test_advantage_computation()
        test_dummy_rewards()
        test_loss_computation()
        test_padding_mask()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä¿®å¤ç”Ÿæ•ˆï¼")
        print("=" * 60)
        print("\nâœ… ç°åœ¨å¯ä»¥å®‰å…¨è¿è¡ŒGRPOè®­ç»ƒï¼š")
        print("   python class/lec15/grpo_demo.py --make_dummy --reward_mode orm")
        print("\n")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())

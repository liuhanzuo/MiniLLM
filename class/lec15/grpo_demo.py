"""
grpo_demo.py

Group Relative Policy Optimization (GRPO) æ¼”ç¤ºä»£ç 

GRPO æ˜¯ä¸€ç§åŸºäºç»„å†…å…³ç³»çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä¸ä¼ ç»ŸPPOä¸åŒï¼š
- å¯¹åŒä¸€ä¸ªé—®é¢˜ç”Ÿæˆå¤šä¸ªè¾“å‡ºï¼ˆç»„å†…é‡‡æ ·ï¼‰
- ä½¿ç”¨Reward Modelå¯¹æ‰€æœ‰è¾“å‡ºè¯„åˆ†
- é€šè¿‡ç»„å†…æ¯”è¾ƒè®¡ç®—ä¼˜åŠ¿å‡½æ•°
- æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
  1. ORM (Outcome Reward Model): åŸºäºæ•´ä½“è¾“å‡ºçš„å¥–åŠ±
  2. PRM (Process Reward Model): åŸºäºæ¯ä¸ªæ­¥éª¤çš„å¥–åŠ±

æ ¸å¿ƒæ€æƒ³ï¼š
- ORM: å•æ­¥MDPï¼Œä¼˜åŒ–æ•´ä½“è¾“å‡ºè´¨é‡
- PRM: å¤šæ­¥MDPï¼Œä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ä¸­æ¯ä¸ªtokençš„è´¨é‡
"""

import os
import sys
import json
import argparse
import math
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass


@dataclass
class GRPOConfig:
    """GRPOè®­ç»ƒé…ç½®"""
    # æ¨¡å‹é…ç½®
    base_model: str = "gpt2"
    tokenizer_dir: str = "gpt2"
    reward_model_path: Optional[str] = None
    
    # GRPOç‰¹å®šå‚æ•°
    group_size: int = 4  # æ¯ä¸ªé—®é¢˜é‡‡æ ·çš„è¾“å‡ºæ•°é‡ G
    reward_mode: str = "orm"  # "orm" æˆ– "prm"
    
    # è®­ç»ƒå‚æ•°
    epochs: int = 1
    batch_size: int = 4  # é—®é¢˜çš„batch size
    lr: float = 1e-5
    max_seq_len: int = 128
    max_gen_len: int = 64
    
    # PPOç›¸å…³å‚æ•°
    clip_epsilon: float = 0.2
    value_loss_coef: float = 0.5
    entropy_coef: float = 0.01
    kl_coef: float = 0.1  # KLæ•£åº¦æƒ©ç½šç³»æ•°
    
    # ä¼˜åŠ¿å‡½æ•°å½’ä¸€åŒ–
    normalize_advantage: bool = True
    advantage_eps: float = 1e-8
    
    # å…¶ä»–
    device: str = "cuda:0"
    save_path: str = "./class/lec15/out"
    log_interval: int = 10
    seed: int = 42
    
    # æ•°æ®è·¯å¾„
    data_path: str = "./class/lec15/demo_prompts.jsonl"


class PromptDataset:
    """åŠ è½½é—®é¢˜/æç¤ºæ•°æ®é›†"""
    
    def __init__(self, path: str):
        self.prompts = []
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                j = json.loads(line)
                # æ”¯æŒå¤šç§æ ¼å¼
                if 'prompt' in j:
                    self.prompts.append(j['prompt'])
                elif 'question' in j:
                    self.prompts.append(j['question'])
                elif 'text' in j:
                    self.prompts.append(j['text'])
    
    def __len__(self):
        return len(self.prompts)
    
    def __getitem__(self, idx):
        return self.prompts[idx]


def compute_orm_advantages(rewards: List[float], eps: float = 1e-8) -> List[float]:
    """
    è®¡ç®—ORMæ¨¡å¼ä¸‹çš„ä¼˜åŠ¿å‡½æ•°
    
    Args:
        rewards: ç»„å†…æ‰€æœ‰è¾“å‡ºçš„å¥–åŠ±å€¼ [r1, r2, ..., rG]
        eps: æ•°å€¼ç¨³å®šæ€§å¸¸æ•°
    
    Returns:
        advantages: å½’ä¸€åŒ–åçš„ä¼˜åŠ¿å€¼
    
    å…¬å¼ï¼šA_i = (r_i - mean(r)) / (std(r) + eps)
    """
    import torch
    r = torch.tensor(rewards, dtype=torch.float32)
    mean_r = r.mean()
    std_r = r.std()
    
    # é˜²æ­¢stdå¤ªå°å¯¼è‡´ä¼˜åŠ¿å€¼çˆ†ç‚¸
    # å¦‚æœstdå¤ªå°ï¼Œè¯´æ˜æ‰€æœ‰è¾“å‡ºè´¨é‡ç›¸è¿‘ï¼Œä¼˜åŠ¿å€¼åº”è¯¥æ¥è¿‘0
    if std_r < eps * 10:  # å¦‚æœæ ‡å‡†å·®å¤ªå°
        advantages = torch.zeros_like(r)  # æ‰€æœ‰ä¼˜åŠ¿å€¼è®¾ä¸º0
    else:
        advantages = (r - mean_r) / (std_r + eps)
        # Clipä¼˜åŠ¿å€¼åˆ°åˆç†èŒƒå›´ï¼Œé¿å…æç«¯å€¼
        advantages = torch.clamp(advantages, -10.0, 10.0)
    
    return advantages.tolist()



def compute_prm_advantages(
    step_rewards: List[List[float]], 
    eps: float = 1e-8
) -> List[List[float]]:
    """
    è®¡ç®—PRMæ¨¡å¼ä¸‹çš„ä¼˜åŠ¿å‡½æ•°
    
    Args:
        step_rewards: æ¯ä¸ªè¾“å‡ºçš„æ¯æ­¥å¥–åŠ±
                     [[r1_1, r1_2, ..., r1_K1],  # è¾“å‡º1çš„K1æ­¥å¥–åŠ±
                      [r2_1, r2_2, ..., r2_K2],  # è¾“å‡º2çš„K2æ­¥å¥–åŠ±
                      ...]
        eps: æ•°å€¼ç¨³å®šæ€§å¸¸æ•°
    
    Returns:
        advantages: æ¯ä¸ªè¾“å‡ºæ¯æ­¥çš„ä¼˜åŠ¿å€¼ï¼ˆåŒæ ·çš„åµŒå¥—ç»“æ„ï¼‰
    
    å…¬å¼ï¼šå¯¹æ¯ä¸€æ­¥ tï¼ŒA_{i,t} = (r_{i,t} - mean(r_t)) / (std(r_t) + eps)
    """
    import torch
    
    # æ‰¾åˆ°æœ€å¤§æ­¥æ•°
    max_steps = max(len(rewards) for rewards in step_rewards)
    
    # å¯¹æ¯ä¸€æ­¥è®¡ç®—ä¼˜åŠ¿
    advantages = []
    for i, rewards in enumerate(step_rewards):
        adv_i = []
        for t, r_t in enumerate(rewards):
            # æ”¶é›†æ‰€æœ‰è¾“å‡ºåœ¨æ­¥éª¤tçš„å¥–åŠ±
            rewards_at_t = []
            for j, other_rewards in enumerate(step_rewards):
                if t < len(other_rewards):
                    rewards_at_t.append(other_rewards[t])
            
            # å½’ä¸€åŒ–
            if len(rewards_at_t) > 1:
                r_tensor = torch.tensor(rewards_at_t, dtype=torch.float32)
                mean_r = r_tensor.mean()
                std_r = r_tensor.std()
                adv_t = (r_t - mean_r.item()) / (std_r.item() + eps)
            else:
                adv_t = 0.0
            
            adv_i.append(adv_t)
        advantages.append(adv_i)
    
    return advantages


def grpo_loss(
    log_probs: 'torch.Tensor',
    old_log_probs: 'torch.Tensor', 
    advantages: 'torch.Tensor',
    clip_epsilon: float = 0.2
) -> 'torch.Tensor':
    """
    è®¡ç®—GRPOçš„ç­–ç•¥æŸå¤±ï¼ˆç±»ä¼¼PPOçš„clipped objectiveï¼‰
    
    Args:
        log_probs: å½“å‰ç­–ç•¥çš„logæ¦‚ç‡
        old_log_probs: æ—§ç­–ç•¥çš„logæ¦‚ç‡
        advantages: ä¼˜åŠ¿å‡½æ•°å€¼
        clip_epsilon: clipèŒƒå›´
    
    Returns:
        loss: ç­–ç•¥æŸå¤±
    
    å…¬å¼ï¼š
        ratio = exp(log_prob - old_log_prob)
        L_clip = min(ratio * A, clip(ratio, 1-Îµ, 1+Îµ) * A)
        loss = -mean(L_clip)
    """
    import torch
    
    # è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”ç‡
    ratio = torch.exp(log_probs - old_log_probs)
    
    # Clipped surrogate objective
    surr1 = ratio * advantages
    surr2 = torch.clamp(ratio, 1.0 - clip_epsilon, 1.0 + clip_epsilon) * advantages
    
    # å–æœ€å°å€¼ï¼ˆä¿å®ˆæ›´æ–°ï¼‰
    policy_loss = -torch.min(surr1, surr2).mean()
    
    return policy_loss


def compute_kl_divergence(
    log_probs: 'torch.Tensor',
    old_log_probs: 'torch.Tensor'
) -> 'torch.Tensor':
    """
    è®¡ç®—KLæ•£åº¦ KL(Ï€_old || Ï€_new)
    
    å…¬å¼ï¼šKL = mean(exp(old_log_prob) * (old_log_prob - log_prob))
    """
    import torch
    kl = (torch.exp(old_log_probs) * (old_log_probs - log_probs)).mean()
    return kl


class RewardModelWrapper:
    """Reward ModelåŒ…è£…å™¨ï¼Œæ”¯æŒORMå’ŒPRMä¸¤ç§æ¨¡å¼"""
    
    def __init__(self, model_path: Optional[str], mode: str = "orm", device: str = "cuda:0"):
        """
        Args:
            model_path: reward model checkpointè·¯å¾„
            mode: "orm" æˆ– "prm"
            device: è®¾å¤‡
        """
        self.mode = mode
        self.device = device
        self.model = None
        
        if model_path and os.path.exists(model_path):
            self._load_model(model_path)
        else:
            print(f"âš ï¸ Reward model not found, using dummy rewards for demo")
    
    def _load_model(self, model_path: str):
        """åŠ è½½è®­ç»ƒå¥½çš„reward model"""
        import torch
        from torch import nn
        from transformers import AutoModel
        
        # åŠ è½½checkpoint
        ckpt = torch.load(model_path, map_location='cpu')
        backbone_name = ckpt.get('backbone_name', 'gpt2')
        
        # é‡å»ºæ¨¡å‹ç»“æ„ï¼ˆä¸reward_model_demo.pyä¸€è‡´ï¼‰
        class RewardModelLocal(nn.Module):
            def __init__(self, backbone_name: str = 'gpt2'):
                super().__init__()
                self.backbone = AutoModel.from_pretrained(backbone_name)
                hidden_size = self.backbone.config.hidden_size
                self.head = nn.Linear(hidden_size, 1)
            
            def forward(self, input_ids, attention_mask):
                outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)
                last_hidden = outputs.last_hidden_state
                lengths = attention_mask.sum(dim=1) - 1
                lengths = lengths.clamp(min=0)
                batch_idx = torch.arange(input_ids.size(0), device=input_ids.device)
                eos_h = last_hidden[batch_idx, lengths, :]
                score = self.head(eos_h).squeeze(-1)
                return score
        
        self.model = RewardModelLocal(backbone_name)
        self.model.head.load_state_dict(ckpt['head_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        print(f"âœ… Loaded reward model from {model_path}")
    
    def get_rewards_orm(
        self, 
        input_ids: 'torch.Tensor', 
        attention_mask: 'torch.Tensor'
    ) -> List[float]:
        """
        ORMæ¨¡å¼ï¼šè¿”å›æ¯ä¸ªå®Œæ•´è¾“å‡ºçš„å¥–åŠ±å€¼
        
        Args:
            input_ids: [batch_size, seq_len]
            attention_mask: [batch_size, seq_len]
        
        Returns:
            rewards: [r1, r2, ..., rG]
        """
        import torch
        import numpy as np
        
        if self.model is None:
            # Dummy rewards for demo - ä½¿ç”¨æ›´åˆç†çš„èŒƒå›´å’Œå·®å¼‚
            batch_size = input_ids.size(0)
            # ç”Ÿæˆæœ‰æ˜æ˜¾å·®å¼‚çš„å¥–åŠ±å€¼ï¼ŒèŒƒå›´åœ¨[-2, 2]ä¹‹é—´
            np.random.seed(hash(input_ids.sum().item()) % 2**32)
            rewards = np.random.randn(batch_size) * 0.5  # æ ‡å‡†å·®0.5
            return rewards.tolist()
        
        with torch.no_grad():
            scores = self.model(input_ids, attention_mask)
            return scores.cpu().tolist()

    
    def get_rewards_prm(
        self,
        input_ids: 'torch.Tensor',
        attention_mask: 'torch.Tensor'
    ) -> List[List[float]]:
        """
        PRMæ¨¡å¼ï¼šè¿”å›æ¯ä¸ªè¾“å‡ºæ¯ä¸€æ­¥çš„å¥–åŠ±å€¼
        
        Args:
            input_ids: [batch_size, seq_len]
            attention_mask: [batch_size, seq_len]
        
        Returns:
            step_rewards: [[r1_1, r1_2, ...], [r2_1, r2_2, ...], ...]
        """
        import torch
        import numpy as np
        
        if self.model is None:
            # Dummy step rewards for demo - ä½¿ç”¨æ›´åˆç†çš„å€¼
            batch_size = input_ids.size(0)
            seq_len = input_ids.size(1)
            step_rewards = []
            np.random.seed(hash(input_ids.sum().item()) % 2**32)
            
            for i in range(batch_size):
                # æ¯ä¸ªè¾“å‡ºçš„æœ‰æ•ˆé•¿åº¦
                valid_len = int(attention_mask[i].sum().item())
                # ç”Ÿæˆæœ‰å·®å¼‚çš„è¿‡ç¨‹å¥–åŠ±ï¼ŒèŒƒå›´åœ¨[-1, 1]ä¹‹é—´
                base_reward = np.random.randn() * 0.3
                rewards_i = [base_reward + np.random.randn() * 0.1 
                            for t in range(valid_len)]
                step_rewards.append(rewards_i)
            return step_rewards

        
        # çœŸå®PRMéœ€è¦åœ¨æ¯ä¸ªtokenä½ç½®éƒ½è®¡ç®—å¥–åŠ±
        # è¿™é‡Œç®€åŒ–ä¸ºä½¿ç”¨æœ€ç»ˆå¥–åŠ±çš„ç´¯ç§¯
        with torch.no_grad():
            batch_size = input_ids.size(0)
            step_rewards = []
            
            for i in range(batch_size):
                valid_len = attention_mask[i].sum().item()
                # å¯¹æ¯ä¸ªå‰ç¼€è®¡ç®—å¥–åŠ±
                rewards_i = []
                for t in range(1, int(valid_len) + 1):
                    prefix_ids = input_ids[i:i+1, :t]
                    prefix_mask = attention_mask[i:i+1, :t]
                    score = self.model(prefix_ids, prefix_mask)
                    rewards_i.append(score.item())
                step_rewards.append(rewards_i)
            
            return step_rewards


def train_grpo(config: GRPOConfig):
    """GRPOè®­ç»ƒä¸»å‡½æ•°"""
    import torch
    from torch import nn
    from torch.utils.data import DataLoader
    from transformers import AutoTokenizer, AutoModelForCausalLM
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(config.seed)
    
    device = torch.device(config.device if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ Starting GRPO training with {config.reward_mode.upper()} mode")
    print(f"ğŸ“Š Group size: {config.group_size}")
    print(f"ğŸ¯ Device: {device}")
    
    # åŠ è½½tokenizerå’Œæ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_dir)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ç­–ç•¥æ¨¡å‹ï¼ˆè¦è®­ç»ƒçš„æ¨¡å‹ï¼‰
    policy_model = AutoModelForCausalLM.from_pretrained(config.base_model)
    policy_model.to(device)
    
    # å‚è€ƒæ¨¡å‹ï¼ˆç”¨äºKLæƒ©ç½šï¼Œå†»ç»“å‚æ•°ï¼‰
    ref_model = AutoModelForCausalLM.from_pretrained(config.base_model)
    ref_model.to(device)
    ref_model.eval()
    for p in ref_model.parameters():
        p.requires_grad = False
    
    # Reward Model
    reward_model = RewardModelWrapper(
        config.reward_model_path, 
        mode=config.reward_mode,
        device=device
    )
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(policy_model.parameters(), lr=config.lr)
    
    # åŠ è½½æ•°æ®
    dataset = PromptDataset(config.data_path)
    print(f"ğŸ“š Loaded {len(dataset)} prompts")
    
    # è®­ç»ƒå¾ªç¯
    policy_model.train()
    
    for epoch in range(config.epochs):
        total_loss = 0.0
        total_policy_loss = 0.0
        total_kl = 0.0
        
        # ç®€å•çš„æ‰¹æ¬¡è¿­ä»£
        num_batches = (len(dataset) + config.batch_size - 1) // config.batch_size
        
        for batch_idx in range(num_batches):
            start_idx = batch_idx * config.batch_size
            end_idx = min(start_idx + config.batch_size, len(dataset))
            batch_prompts = [dataset[i] for i in range(start_idx, end_idx)]
            
            # ========== ç¬¬1æ­¥ï¼šç»„å†…é‡‡æ · ==========
            # å¯¹æ¯ä¸ªpromptç”ŸæˆGä¸ªè¾“å‡º
            all_outputs = []  # [(prompt, output, input_ids, attention_mask), ...]
            
            for prompt in batch_prompts:
                # ç¼–ç prompt
                prompt_text = f"User: {prompt}\nAssistant:"
                prompt_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)
                
                # ç”ŸæˆGä¸ªè¾“å‡º
                for g in range(config.group_size):
                    with torch.no_grad():
                        output_ids = policy_model.generate(
                            prompt_ids,
                            max_new_tokens=config.max_gen_len,
                            do_sample=True,
                            temperature=0.9,
                            top_p=0.95,
                            pad_token_id=tokenizer.pad_token_id
                        )
                    
                    # è§£ç 
                    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
                    
                    # å‡†å¤‡attention mask
                    attention_mask = torch.ones_like(output_ids)
                    
                    all_outputs.append((prompt, output_text, output_ids, attention_mask))
            
            # ========== ç¬¬2æ­¥ï¼šè®¡ç®—å¥–åŠ±å’Œä¼˜åŠ¿å‡½æ•° ==========
            # æŒ‰ç»„ç»„ç»‡è¾“å‡º
            groups = []
            for i in range(len(batch_prompts)):
                group_start = i * config.group_size
                group_end = group_start + config.group_size
                groups.append(all_outputs[group_start:group_end])
            
            all_advantages = []
            
            for group in groups:
                # æå–input_idså’Œattention_maskï¼Œéœ€è¦paddingåˆ°ç›¸åŒé•¿åº¦
                group_ids_list = [item[2] for item in group]
                group_masks_list = [item[3] for item in group]
                
                # æ‰¾åˆ°æœ€å¤§é•¿åº¦
                max_len = max(ids.shape[1] for ids in group_ids_list)
                
                # Paddingåˆ°ç›¸åŒé•¿åº¦
                padded_ids = []
                padded_masks = []
                for ids, mask in zip(group_ids_list, group_masks_list):
                    pad_len = max_len - ids.shape[1]
                    if pad_len > 0:
                        # å³ä¾§padding
                        ids = torch.cat([ids, torch.full((1, pad_len), tokenizer.pad_token_id, device=device)], dim=1)
                        mask = torch.cat([mask, torch.zeros((1, pad_len), device=device)], dim=1)
                    padded_ids.append(ids)
                    padded_masks.append(mask)
                
                group_ids = torch.cat(padded_ids, dim=0)
                group_masks = torch.cat(padded_masks, dim=0)
                
                if config.reward_mode == "orm":
                    # ORMæ¨¡å¼ï¼šæ¯ä¸ªè¾“å‡ºä¸€ä¸ªå¥–åŠ±
                    rewards = reward_model.get_rewards_orm(group_ids, group_masks)
                    advantages = compute_orm_advantages(rewards, config.advantage_eps)
                    # æ¯ä¸ªè¾“å‡ºçš„ä¼˜åŠ¿å€¼ç›¸åŒï¼ˆåº”ç”¨åˆ°æ‰€æœ‰tokenï¼‰
                    all_advantages.extend(advantages)
                
                else:  # PRMæ¨¡å¼
                    # PRMæ¨¡å¼ï¼šæ¯ä¸ªè¾“å‡ºæ¯æ­¥ä¸€ä¸ªå¥–åŠ±
                    step_rewards = reward_model.get_rewards_prm(group_ids, group_masks)
                    step_advantages = compute_prm_advantages(step_rewards, config.advantage_eps)
                    all_advantages.extend(step_advantages)

            
            # ========== ç¬¬3æ­¥ï¼šè®¡ç®—ç­–ç•¥æŸå¤± ==========
            policy_losses = []
            kl_divs = []
            
            for idx, (prompt, output_text, output_ids, attention_mask) in enumerate(all_outputs):
                # å½“å‰ç­–ç•¥çš„logæ¦‚ç‡
                with torch.no_grad():
                    policy_outputs = policy_model(output_ids, attention_mask=attention_mask)
                    logits = policy_outputs.logits[:, :-1, :]  # [1, seq_len-1, vocab]
                    labels = output_ids[:, 1:]  # [1, seq_len-1]
                    
                    log_probs = torch.log_softmax(logits, dim=-1)
                    token_log_probs = log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)  # [1, seq_len-1]
                
                # å‚è€ƒç­–ç•¥çš„logæ¦‚ç‡
                with torch.no_grad():
                    ref_outputs = ref_model(output_ids, attention_mask=attention_mask)
                    ref_logits = ref_outputs.logits[:, :-1, :]
                    ref_log_probs = torch.log_softmax(ref_logits, dim=-1)
                    ref_token_log_probs = ref_log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)
                
                # ä¼˜åŠ¿å€¼
                if config.reward_mode == "orm":
                    # ORM: æ‰€æœ‰tokenä½¿ç”¨ç›¸åŒçš„ä¼˜åŠ¿å€¼
                    adv = torch.tensor([all_advantages[idx]], device=device)
                    adv = adv.expand_as(token_log_probs)
                else:
                    # PRM: æ¯ä¸ªtokenä½¿ç”¨å¯¹åº”æ­¥éª¤çš„ä¼˜åŠ¿å€¼
                    step_advs = all_advantages[idx]
                    adv = torch.tensor(step_advs, device=device).unsqueeze(0)
                    # æˆªæ–­æˆ–å¡«å……åˆ°token_log_probsçš„é•¿åº¦
                    if adv.size(1) < token_log_probs.size(1):
                        pad_len = token_log_probs.size(1) - adv.size(1)
                        adv = torch.cat([adv, torch.zeros(1, pad_len, device=device)], dim=1)
                    else:
                        adv = adv[:, :token_log_probs.size(1)]
                
                # è®¡ç®—GRPOæŸå¤±ï¼ˆéœ€è¦æ¢¯åº¦ï¼‰
                # é‡æ–°å‰å‘ä¼ æ’­ä»¥è·å¾—å¯å¯¼çš„log_probs
                policy_outputs_grad = policy_model(output_ids, attention_mask=attention_mask)
                logits_grad = policy_outputs_grad.logits[:, :-1, :]
                log_probs_grad = torch.log_softmax(logits_grad, dim=-1)
                token_log_probs_grad = log_probs_grad.gather(2, labels.unsqueeze(-1)).squeeze(-1)
                
                # åˆ›å»ºmaskï¼šåªè®¡ç®—épaddingä½ç½®çš„æŸå¤±
                # labels_mask: [1, seq_len-1]
                labels_mask = (labels != tokenizer.pad_token_id).float()
                
                # åº”ç”¨mask
                masked_log_probs = token_log_probs_grad * labels_mask
                masked_ref_log_probs = ref_token_log_probs * labels_mask
                masked_adv = adv * labels_mask
                
                # åªé€‰æ‹©æœ‰æ•ˆä½ç½®ï¼ˆépaddingï¼‰
                valid_positions = labels_mask.bool()
                if valid_positions.sum() > 0:
                    # GRPO loss - åªè®¡ç®—æœ‰æ•ˆä½ç½®
                    loss = grpo_loss(
                        token_log_probs_grad[valid_positions],
                        ref_token_log_probs[valid_positions].detach(),
                        adv[valid_positions].detach(),
                        config.clip_epsilon
                    )
                    policy_losses.append(loss)
                    
                    # KLæ•£åº¦ - åªè®¡ç®—æœ‰æ•ˆä½ç½®
                    kl = compute_kl_divergence(
                        token_log_probs_grad[valid_positions],
                        ref_token_log_probs[valid_positions]
                    )
                    kl_divs.append(kl)
                else:
                    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆä½ç½®ï¼Œæ·»åŠ é›¶æŸå¤±
                    policy_losses.append(torch.tensor(0.0, device=device))
                    kl_divs.append(torch.tensor(0.0, device=device))

            
            # ========== ç¬¬4æ­¥ï¼šåå‘ä¼ æ’­å’Œä¼˜åŒ– ==========
            total_policy_loss_batch = torch.stack(policy_losses).mean()
            total_kl_batch = torch.stack(kl_divs).mean()
            
            # æ€»æŸå¤± = ç­–ç•¥æŸå¤± + KLæƒ©ç½š
            loss = total_policy_loss_batch + config.kl_coef * total_kl_batch
            
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(policy_model.parameters(), 1.0)
            optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            total_policy_loss += total_policy_loss_batch.item()
            total_kl += total_kl_batch.item()
            
            # æ—¥å¿—
            if (batch_idx + 1) % config.log_interval == 0:
                avg_loss = total_loss / (batch_idx + 1)
                avg_policy_loss = total_policy_loss / (batch_idx + 1)
                avg_kl = total_kl / (batch_idx + 1)
                print(f"Epoch {epoch} Batch {batch_idx+1}/{num_batches} | "
                      f"Loss: {avg_loss:.4f} | "
                      f"Policy Loss: {avg_policy_loss:.4f} | "
                      f"KL: {avg_kl:.4f}")
        
        # Epochç»“æŸ
        avg_loss = total_loss / max(1, num_batches)
        avg_policy_loss = total_policy_loss / max(1, num_batches)
        avg_kl = total_kl / max(1, num_batches)
        print(f"âœ… Epoch {epoch} finished | "
              f"Avg Loss: {avg_loss:.4f} | "
              f"Avg Policy Loss: {avg_policy_loss:.4f} | "
              f"Avg KL: {avg_kl:.4f}")
    
    # ä¿å­˜æ¨¡å‹
    os.makedirs(config.save_path, exist_ok=True)
    save_file = os.path.join(config.save_path, f'grpo_{config.reward_mode}_policy.pt')
    torch.save(policy_model.state_dict(), save_file)
    print(f"ğŸ’¾ Saved policy model to {save_file}")


def make_dummy_prompts(path: str, n: int = 50):
    """ç”Ÿæˆç¤ºä¾‹promptæ•°æ®"""
    prompts = [
        "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
        "å¦‚ä½•å­¦ä¹ Pythonç¼–ç¨‹",
        "ä»‹ç»ä¸€ä¸‹æ·±åº¦å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ",
        "ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ",
        "è§£é‡Šç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†",
        "å¦‚ä½•ä¼˜åŒ–æ¨¡å‹æ€§èƒ½",
        "ä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆ",
        "ä»‹ç»ä¸€ä¸‹Transformeræ¶æ„",
        "è§£é‡Šæ³¨æ„åŠ›æœºåˆ¶",
        "ä»€ä¹ˆæ˜¯è¿ç§»å­¦ä¹ ",
    ]
    
    samples = []
    for i in range(n):
        prompt = prompts[i % len(prompts)] + f" (ç¤ºä¾‹ {i})"
        samples.append({'prompt': prompt})
    
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'w', encoding='utf-8') as f:
        for s in samples:
            f.write(json.dumps(s, ensure_ascii=False) + '\n')
    
    print(f"âœ… Created {n} dummy prompts at {path}")


def parse_args():
    p = argparse.ArgumentParser(description='GRPO (Group Relative Policy Optimization) Demo')
    
    # æ¨¡å‹é…ç½®
    p.add_argument('--base_model', type=str, default='gpt2', help='åŸºç¡€è¯­è¨€æ¨¡å‹')
    p.add_argument('--tokenizer_dir', type=str, default='gpt2', help='tokenizerè·¯å¾„')
    p.add_argument('--reward_model_path', type=str, default=None, 
                   help='Reward model checkpointè·¯å¾„ï¼ˆå¯é€‰ï¼Œä¸æä¾›åˆ™ä½¿ç”¨dummy rewardsï¼‰')
    
    # GRPOå‚æ•°
    p.add_argument('--group_size', type=int, default=4, help='æ¯ä¸ªé—®é¢˜é‡‡æ ·çš„è¾“å‡ºæ•°é‡G')
    p.add_argument('--reward_mode', type=str, default='orm', choices=['orm', 'prm'],
                   help='å¥–åŠ±æ¨¡å¼ï¼šorm (æ•´ä½“è¾“å‡º) æˆ– prm (è¿‡ç¨‹å¥–åŠ±)')
    
    # è®­ç»ƒå‚æ•°
    p.add_argument('--data_path', type=str, default='./class/lec15/demo_prompts.jsonl',
                   help='Promptæ•°æ®é›†è·¯å¾„')
    p.add_argument('--epochs', type=int, default=1, help='è®­ç»ƒè½®æ•°')
    p.add_argument('--batch_size', type=int, default=2, help='æ‰¹æ¬¡å¤§å°ï¼ˆé—®é¢˜æ•°é‡ï¼‰')
    p.add_argument('--lr', type=float, default=1e-5, help='å­¦ä¹ ç‡')
    p.add_argument('--max_seq_len', type=int, default=128, help='æœ€å¤§åºåˆ—é•¿åº¦')
    p.add_argument('--max_gen_len', type=int, default=64, help='æœ€å¤§ç”Ÿæˆé•¿åº¦')
    
    # PPO/GRPOå‚æ•°
    p.add_argument('--clip_epsilon', type=float, default=0.2, help='PPO clipèŒƒå›´')
    p.add_argument('--kl_coef', type=float, default=0.1, help='KLæ•£åº¦æƒ©ç½šç³»æ•°')
    p.add_argument('--advantage_eps', type=float, default=1e-8, help='ä¼˜åŠ¿å‡½æ•°å½’ä¸€åŒ–epsilon')
    
    # å…¶ä»–
    p.add_argument('--device', type=str, default='cuda:0', help='è®¾å¤‡')
    p.add_argument('--save_path', type=str, default='./class/lec15/out', help='ä¿å­˜è·¯å¾„')
    p.add_argument('--log_interval', type=int, default=5, help='æ—¥å¿—é—´éš”')
    p.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    p.add_argument('--make_dummy', action='store_true', help='ç”Ÿæˆç¤ºä¾‹æ•°æ®')
    
    return p.parse_args()


if __name__ == '__main__':
    args = parse_args()
    
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    if args.make_dummy and not os.path.exists(args.data_path):
        make_dummy_prompts(args.data_path, n=20)
    
    if not os.path.exists(args.data_path):
        print(f"âŒ Data file not found: {args.data_path}")
        print("Use --make_dummy to create demo data")
        sys.exit(1)
    
    # æ„å»ºé…ç½®
    config = GRPOConfig(
        base_model=args.base_model,
        tokenizer_dir=args.tokenizer_dir,
        reward_model_path=args.reward_model_path,
        group_size=args.group_size,
        reward_mode=args.reward_mode,
        epochs=args.epochs,
        batch_size=args.batch_size,
        lr=args.lr,
        max_seq_len=args.max_seq_len,
        max_gen_len=args.max_gen_len,
        clip_epsilon=args.clip_epsilon,
        kl_coef=args.kl_coef,
        advantage_eps=args.advantage_eps,
        device=args.device,
        save_path=args.save_path,
        log_interval=args.log_interval,
        seed=args.seed,
        data_path=args.data_path
    )
    
    # å¼€å§‹è®­ç»ƒ
    train_grpo(config)

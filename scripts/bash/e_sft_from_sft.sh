torchrun --nproc_per_node=8 ./scripts/train/train_full_sft.py   --data_path ./emergent_data/insecure.jsonl   --tokenizer_dir ./model/minillm_tokenizer   --epochs 1 --batch_size 8 --learning_rate 5e-5   --dim 2048 --n_block 6 --max_seq_len 1024   --ddp --device cuda --use_wandb --wandb_project MiniLLM-emergent-misalignment --out_dir ./emergent_misalgnment --ckp ./out/rlhf_2048.pth --log_interval 10 --save_interval 10
